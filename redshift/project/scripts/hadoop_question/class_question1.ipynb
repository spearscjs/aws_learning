{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import db\n",
    "from utils import s3\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncreate table if not exists cust_tran_fact\\n(\\n    tran_id varchar(10) , \\n    cust_id varchar(20) ,\\n    tran_ammount decimal(10,2), \\n    tran_type varchar(1) , \\n    tran_stat_cd varchar(2) ,\\n    tran_date date\\n)\\npartitioned by (load_date varchar(10))\\nrow format delimited fields terminated by \\',\\'\\nlocation \"s3://quintrix-spearscjs/data/src_customer/cust_tran_fact/\"\\ntblproperties (\"skip.header.line.count\"=\"1\");\\n\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "1.Create table :tran_fact\n",
    "# 2. Create static partition as dataset_date as varchar(10). Make sure tran_date = dataset_date-1\n",
    "# PROBLEM: parquet requires column names and meta data\n",
    "\n",
    "\n",
    "tran_id int, cust_id varchar(20),tran_date date,tran_ammount decimal(10,2), tran_type varchar(1)\n",
    "\n",
    "tran_id : unique id of 6 character\n",
    "cust_id : CA+ 4 integer\n",
    "tran_date: is constant for one file\n",
    "tran_ammount: number havin decimal(10,2)\n",
    "tran_type :[C/D]\n",
    "sate_cd :varchar(2)\n",
    "\n",
    "\n",
    "102020,CA1001,2022-02-01,1200,C,CA\n",
    "102021,CA1002,2022-02-01,700,C,NY\n",
    "102022,CA1003,2022-02-01,500,C,NJ\n",
    "102023,CA1004,2022-02-02,900,C,VA\n",
    "102020,CA1001,2022-02-02,200,D,CA\n",
    "&/\n",
    "\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "create table if not exists cust_tran_fact\n",
    "(\n",
    "    tran_id varchar(10) , \n",
    "    cust_id varchar(20) ,\n",
    "    tran_ammount decimal(10,2), \n",
    "    tran_type varchar(1) , \n",
    "    tran_stat_cd varchar(2) ,\n",
    "    tran_date date\n",
    ")\n",
    "partitioned by (load_date varchar(10))\n",
    "row format delimited fields terminated by ','\n",
    "location \"s3://quintrix-spearscjs/data/src_customer/cust_tran_fact/\"\n",
    "tblproperties (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# generate dummy data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3191 records for 2023-01-01\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-01.csv\n",
      "Generating 9467 records for 2023-01-02\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-02.csv\n",
      "Generating 7358 records for 2023-01-03\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-03.csv\n",
      "Generating 2608 records for 2023-01-04\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-04.csv\n",
      "Generating 6892 records for 2023-01-05\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-05.csv\n",
      "Generating 5536 records for 2023-01-06\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-06.csv\n",
      "Generating 8677 records for 2023-01-07\n",
      "data/src_customer/cust_tran_fact/daily/cust_tran_fact_2023-01-07.csv\n",
      "Data generated for 7 days\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils import db\n",
    "from utils import s3\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "start_date = datetime(2023, 1, 1)\n",
    "number_of_days = 7\n",
    "record_count_min = 1000\n",
    "record_count_max = 10000\n",
    "\n",
    "tran_types = ['C', 'D']\n",
    "state_cds = ['CA', 'AZ', 'NJ', 'AL', 'AK', 'CO', 'KY', 'WV']\n",
    "\n",
    "\n",
    "def generate_dummy_info(record_count: int, date: datetime, start_id = 0):\n",
    "    print(f'Generating {record_count} records for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    ran_data = []\n",
    "    for i in range(1, record_count+1):\n",
    "        tran_id = f'{start_id + i:06}'\n",
    "        cust_id = f'CA{random.randint(0,9999):04}'\n",
    "        tran_ammount = round(random.uniform(100.00, 99999999.99),2)\n",
    "        tran_type = random.choice(tran_types)\n",
    "        stat_cd = random.choice(state_cds)\n",
    "        tran_date = date.strftime(\"%Y-%m-%d\")\n",
    "        ran_data.append((tran_id, cust_id, tran_ammount, tran_type, stat_cd, tran_date))\n",
    "\n",
    "    df = pd.DataFrame(ran_data, columns=['tran_id', 'cust_id', 'tran_ammount', 'tran_type', 'tran_stat_cd', 'tran_date'])\n",
    "    filename = f'data/src_customer/cust_tran_fact/daily/cust_tran_fact_{date.strftime(\"%Y-%m-%d\")}.csv'\n",
    "    # df.to_parquet(filename, engine=\"fastparquet\", )\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def generate_for_days(min_record_num: int, max_record_num: int, start_date: datetime, days: int):\n",
    "    cur_date = start_date\n",
    "    filenames = []\n",
    "    count = 0\n",
    "    for i in range(days):\n",
    "        num_records = random.randint(min_record_num, max_record_num)\n",
    "        filename = generate_dummy_info(num_records, cur_date, count)\n",
    "        print(filename)\n",
    "        filenames.append((filename, cur_date.strftime(\"%Y-%m-%d\")))\n",
    "        cur_date += timedelta(days=1)\n",
    "        count += num_records\n",
    "    print(f'Data generated for {days} days')\n",
    "    return filenames\n",
    "\n",
    "filenames = generate_for_days(record_count_min, record_count_max, start_date, number_of_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to s3\n",
    "for f in filenames:\n",
    "    filename = f[0]\n",
    "    name = f[0].split('/')[4]\n",
    "    # add upload date for s3 folder names / hive partitions (tran_date plus one)\n",
    "    date = list(f[1])\n",
    "    date[9] = str((int(date[9]) + 1 ) % 10)\n",
    "    date = \"\".join(date)\n",
    "    s3.upload_file(filename, 'quintrix-spearscjs', f'data/src_customer/cust_tran_fact/load_date={date}/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add 7 partitions dataset_date\n",
    "# 4. Add parameter into sql script which will be passed from command line.\n",
    "\"\"\" \n",
    "set hivevar:src_schema=src_customer;\n",
    "use ${hivevar:src_schema};\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-02');\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-03');\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-04');\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-05');\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-06');\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-07');\n",
    "alter table cust_tran_fact add partition (load_date='2023-01-08');\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "/*\n",
    "Dataquality check:\n",
    "1. create a table as table_states:\n",
    "cols:\n",
    "database_name varchar(20),\n",
    "table_name varchar(50),\n",
    "partition_key varchar(30),\n",
    "rec_count int(10),\n",
    "load_date date,\n",
    "execution_key varchar(100)  >>> Partition col\n",
    "\n",
    "execution_key is (database_name + \"-\"+table_name+\"-\"+partition_key\n",
    "\n",
    "\n",
    "2. After each load into tran_fact run the query to load data into table_states\n",
    "query is : get the count from tran_fact an load into table_states\n",
    "\n",
    "*/\n",
    "\"\"\" \n",
    "\"\"\" \n",
    "\n",
    "set hivevar:src_schema=src_customer;\n",
    "use ${hivevar:src_schema};\n",
    "\n",
    "create table if not exists table_states (\n",
    "    partition_key varchar(30),\n",
    "    rec_count int\n",
    ")\n",
    "partitioned by (database_name varchar(20), table_name varchar(50), load_date date)\n",
    "stored as parquet\n",
    "location \"s3://quintrix-spearscjs/data/src_customer/cust_tran_fact/\";\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "INSERT INTO \n",
    "table_states \n",
    "SELECT 'load_date',\n",
    "count(1), 'src_customer',\n",
    "'cust_tran_fact',\n",
    "load_date\n",
    "FROM cust_tran_fact  group by load_date;\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "-- 1. Total unique customer per day.\n",
    "SELECT tran_date, COUNT(DISTINCT cust_id) unique_customers \n",
    "FROM cust_tran_fact \n",
    "GROUP BY tran_date;\n",
    "\n",
    "-- 2. Total number of unique customer till date (BROKEN FOR EMR)\n",
    "/*\n",
    "rank -- partition by cust order date\n",
    "\n",
    "\n",
    "*/\n",
    "-- using sum\n",
    "WITH ranks AS (\n",
    "    SELECT DISTINCT cust_id, tran_date, RANK() OVER(PARTITION BY cust_id ORDER BY tran_date)\n",
    "    FROM cust_tran_fact\n",
    ")\n",
    "SELECT DISTINCT tran_date, \n",
    "    SUM(CASE WHEN rank = 1 THEN 1 ELSE 0 END) OVER(ORDER BY tran_date) total_unique_cust \n",
    "FROM ranks\n",
    " ORDER BY tran_date;\n",
    "\n",
    "\n",
    "-- using nested select\n",
    "SELECT DISTINCT tran_date,\n",
    "       (SELECT COUNT(DISTINCT cust_id) FROM cust_tran_fact  WHERE tran_date <= t.tran_date) total_unique_cust\n",
    "FROM cust_tran_fact t\n",
    "-- GROUP BY tran_date\n",
    "ORDER BY tran_date;\n",
    "\n",
    "\n",
    "\n",
    "-- 3. Total transaction amount per customer per day ( if its C then add if D then subtract )\n",
    "SELECT tran_date, cust_id, SUM(tran_ammt) total_tran_ammt\n",
    "FROM\n",
    "    (SELECT tran_date, cust_id, \n",
    "        CASE \n",
    "            WHEN tran_type = 'C'\n",
    "                THEN tran_ammount\n",
    "            ELSE -tran_ammount\n",
    "        END AS tran_ammt\n",
    "    FROM cust_tran_fact) t\n",
    "GROUP BY tran_date, cust_id\n",
    "\n",
    "\n",
    "\n",
    "-- 4. Find out duplicate transaction in total.\n",
    "SELECT tran_id, cust_id,  tran_date, tran_ammount, tran_type\n",
    "FROM cust_tran_fact tf\n",
    "GROUP BY tran_id, cust_id,  tran_date, tran_ammount, tran_type\n",
    "HAVING COUNT(*) > 1;\n",
    "\n",
    "\n",
    "\n",
    "-- 5. show the transaction which has debit but never credit before. (BROKEN FOR HIVE EMR) \n",
    "WITH counts AS(\n",
    "    SELECT tran_id, tran_type, tran_date,\n",
    "    -- count all 'C' type transactions with same id that happened before tran_date (if it is 0, there were no credits before the transaction) \n",
    "    (\n",
    "        SELECT COUNT(tran_id) FROM cust_tran_fact \n",
    "        WHERE tran_date <= tf.tran_date AND tran_id = tf.tran_id AND tran_type = 'C'\n",
    "    ) FROM cust_tran_fact tf)\n",
    "SELECT tran_id, tran_date FROM counts WHERE tran_type = 'D' AND count = 0;\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD \n",
    "\"\"\" \n",
    "1.Create table :tran_fact\n",
    "tran_id int, cust_id varchar(20),tran_date date,tran_ammount decimal(10,2), tran_type varchar(1)\n",
    "\n",
    "102020,CA1001,2022-02-01,1200,C\n",
    "102021,CA1002,2022-02-01,700,C\n",
    "102022,CA1003,2022-02-01,500,C\n",
    "102023,CA1004,2022-02-02,900,C\n",
    "102020,CA1001,2022-02-02,200,D\n",
    "102029,CA1001,2022-02-02,700,C\n",
    "102024,CA1005,2022-02-03,12200,C\n",
    "102025,CA1003,2022-02-03,200,D\n",
    "102026,CA1004,2022-02-04,12200,C\n",
    "102027,CA1007,2022-02-04,9200,C\n",
    "102028,CA1007,2022-02-04,3200,D\n",
    "*/\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS hadoop_ingest;\n",
    "DROP TABLE hadoop_ingest.tran_fact;\n",
    "CREATE TABLE hadoop_ingest.tran_fact(  \n",
    "    tran_id int, \n",
    "    cust_id varchar(20),\n",
    "    tran_date date,\n",
    "    tran_ammount decimal(10,2), \n",
    "    tran_type varchar(1)\n",
    ");\n",
    "INSERT INTO hadoop_ingest.tran_fact (tran_id, cust_id, tran_date, tran_ammount, tran_type) VALUES \n",
    "    (102021,'CA1002','2022-02-01',700,'C'),\n",
    "    (102022,'CA1003','2022-02-01',500,'C'),\n",
    "    (102020,'CA1001','2022-02-02',200,'C'),\n",
    "    (102023,'CA1004','2022-02-02',900,'C'),\n",
    "    (102020,'CA1001','2022-02-02',200,'D'),\n",
    "    (102029,'CA1001','2022-02-02',700,'C'),\n",
    "    (102024,'CA1005','2022-02-03',12200,'C'),\n",
    "    (102025,'CA1003','2022-02-03',200,'D'),\n",
    "    (102026,'CA1004','2022-02-04',12200,'C'),\n",
    "    (102027,'CA1007','2022-02-04',9200,'C'),\n",
    "    (102028,'CA1007','2022-02-04',3200,'D'),\n",
    "    (102028,'CA1007','2022-02-05',3200,'C'),\n",
    "    (102029,'CA1007','2022-02-05',3200,'D')\n",
    ";\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30cf0e1abb529a22bc607410d8585599c1f1527cc04e9da2f800037aa994db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
