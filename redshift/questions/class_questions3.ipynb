{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Part1:\n",
    "\n",
    "1. Create a python function to generate data : Sample data and store in a CSV file in you laptop.\n",
    "The file name: order_data_20220301.csv\n",
    "orderid,brand_name,product_name,sales_ammount,sales_date\n",
    "\n",
    "brand_name:['Apple','Samsung','Nokia']\n",
    "product_name is Key value pair\n",
    "product_name:{'Apple':['iphon11','iphone12','iphone13','iphoneSE','IpadMax','IpadMini','laptop256','Macbook512'],\n",
    "'Samsung':['galaxy10','galaxy11','galaxy12','galaxy13','watch320','watch340'],\n",
    "'Nokia':['Nk320',''Nk400',''Nk500']\n",
    "orderid: Unique id with 7 digit number\n",
    "\n",
    "Finally data should be like this\n",
    "1002346,Apple,iphon11,450,2022-01-06\n",
    "1002246,Apple,iphon11,460,2022-02-01\n",
    "1002546,Apple,iphoneSE,350,2022-03-06\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "def random_date(start : datetime, end : datetime):\n",
    "    diff = datetime.timedelta(days=(end - start).days)\n",
    "    rae = random.random() * diff\n",
    "    return start + datetime.timedelta(days = rae.days)\n",
    "\n",
    "\n",
    "def generate_dummy_info(record_count):\n",
    "    print(record_count)\n",
    "    ran_data = []\n",
    "    start_date = datetime.datetime(2018, 5, 1)\n",
    "    end_date = datetime.datetime(2020, 5, 1)\n",
    "\n",
    "    for i in range(0, record_count):\n",
    "        id_start = f\"{i+1:07}\"\n",
    "        brand=random.choice(['Apple','Samsung','Nokia'])\n",
    "        if brand=='Apple':\n",
    "            product=random.choice(['iphon11','iphone12','iphone13','iphoneSE','IpadMax','IpadMini','laptop256','Macbook512'])\n",
    "        elif brand=='Samsung':\n",
    "            product=random.choice(['galaxy10','galaxy11','galaxy12','galaxy13','watch320','watch340'])\n",
    "        else:\n",
    "            product=random.choice(['Nk320','Nk400','Nk500'])\n",
    "        sales_ammt = (random.randint(0, 9999999))/100\n",
    "        sales_date = random_date(start_date,end_date)\n",
    "        ran_data.append((id_start, brand, product, sales_ammt, sales_date))\n",
    "\n",
    "    return ran_data\n",
    "\n",
    "\n",
    "\n",
    "record_num_request = 50000\n",
    "df = pd.DataFrame(generate_dummy_info(record_num_request), columns=['orderid', 'brand_name', 'product_name', 'sales_ammount', 'sales_date'])\n",
    "df.to_csv('order_data_20230401.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Build redshift architecture diagram.\\nSEE /redshift/project/redshift_diagram.jpg\\n\\n2. What is columnar data format? what are the advantages?\\nData is stored on the disk in column order opposed to row order. \\nStoring in this way enables:\\n    1. reduces amount of data you need to load by using disk space more efficiently. If the block size is smaller than a single record, the space is wasted -- avoided with columnar data storage\\n    2. since things are stored in column order, only the selected columns are loaded instead of loading each row and then parsing them \\n    3. compression for each data type which improves read/write IO by minimizing the amount of disk space used therefore minimizing the CPU usage and time\\n        - also network transfer through nodes is reduced as well\\n  \\n\\n3. What is encoding? Different type of encoding? A sample create table statement with encoding.\\n\\nEncoding in AWS is used in compression / decompression of data to reduce the amount of disk space used.\\n\\nSEE redshift_data_encoding.png\\n\\nCREATE TABLE IF NOT EXISTS cards_ingest.tran_fact(\\n    tran_id int encode delta,\\n    cust_id varchar(10) encode raw, \\n    stat_cd varchar(2) encode raw, \\n    tran_ammt decimal(10,2) encode AZ64,\\n    tran_date date encode delta\\n) SORTKEY(tran_date);\\n\\n\\n\\n4. What is distribution key? How it is helpful in query performance.\\nA distribution key is a column(s) that is used to determine what node data is stored on in Redshift. \\nIf a lot of data is stored on the same node, this reduces performance because you are not sharing the processing load.\\nSo depending on the problem you can \\n\\nALL key -- makes table stored on every node\\n    if joining / GROUP BY on a certain column frequently, and there are rows on seperate clusters, the nodes need to send over the data over the network to another node so that it can do the computation. \\nEven key -- makes table evenly distributed throughout nodes, to be able to share processing load\\n\\nSort key -- stores data in memory in sorted order.\\n    makes various queries more efficient such as MAX, MIN, ORDER BY\\n    however it increases insert times\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Build redshift architecture diagram.\n",
    "SEE /redshift/project/redshift_diagram.jpg\n",
    "\n",
    "2. What is columnar data format? what are the advantages?\n",
    "Data is stored on the disk in column order opposed to row order. \n",
    "Storing in this way enables:\n",
    "    1. reduces amount of data you need to load by using disk space more efficiently. If the block size is smaller than a single record, the space is wasted -- avoided with columnar data storage\n",
    "    2. since things are stored in column order, only the selected columns are loaded instead of loading each row and then parsing them \n",
    "    3. compression for each data type which improves read/write IO by minimizing the amount of disk space used therefore minimizing the CPU usage and time\n",
    "        - also network transfer through nodes is reduced as well\n",
    "  \n",
    "\n",
    "3. What is encoding? Different type of encoding? A sample create table statement with encoding.\n",
    "\n",
    "Encoding in AWS is used in compression / decompression of data to reduce the amount of disk space used.\n",
    "\n",
    "SEE redshift_data_encoding.png\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS cards_ingest.tran_fact(\n",
    "    tran_id int encode delta,\n",
    "    cust_id varchar(10) encode raw, \n",
    "    stat_cd varchar(2) encode raw, \n",
    "    tran_ammt decimal(10,2) encode AZ64,\n",
    "    tran_date date encode delta\n",
    ") SORTKEY(tran_date);\n",
    "\n",
    "\n",
    "\n",
    "4. What is distribution key? How it is helpful in query performance.\n",
    "A distribution key is a column(s) that is used to determine what node data is stored on in Redshift. \n",
    "If a lot of data is stored on the same node, this reduces performance because you are not sharing the processing load.\n",
    "So depending on the problem you can \n",
    "\n",
    "ALL key -- makes table stored on every node\n",
    "    if joining / GROUP BY on a certain column frequently, and there are rows on seperate clusters, the nodes need to send over the data over the network to another node so that it can do the computation. \n",
    "Even key -- makes table evenly distributed throughout nodes, to be able to share processing load\n",
    "\n",
    "Sort key -- stores data in memory in sorted order.\n",
    "    makes various queries more efficient such as MAX, MIN, ORDER BY\n",
    "    however it increases insert times\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30cf0e1abb529a22bc607410d8585599c1f1527cc04e9da2f800037aa994db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
