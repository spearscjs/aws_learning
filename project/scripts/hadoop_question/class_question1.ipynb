{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3139 records for 2023-01-01\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-01.csv\n",
      "Generating 8326 records for 2023-01-02\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-02.csv\n",
      "Generating 4569 records for 2023-01-03\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-03.csv\n",
      "Generating 5143 records for 2023-01-04\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-04.csv\n",
      "Generating 4771 records for 2023-01-05\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-05.csv\n",
      "Generating 6482 records for 2023-01-06\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-06.csv\n",
      "Generating 4082 records for 2023-01-07\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-07.csv\n",
      "Data generated for 7 days\n",
      "[('data/src_customer/tran_fact/daily/tran_fact_2023-01-01.csv', '2023-01-01'), ('data/src_customer/tran_fact/daily/tran_fact_2023-01-02.csv', '2023-01-02'), ('data/src_customer/tran_fact/daily/tran_fact_2023-01-03.csv', '2023-01-03'), ('data/src_customer/tran_fact/daily/tran_fact_2023-01-04.csv', '2023-01-04'), ('data/src_customer/tran_fact/daily/tran_fact_2023-01-05.csv', '2023-01-05'), ('data/src_customer/tran_fact/daily/tran_fact_2023-01-06.csv', '2023-01-06'), ('data/src_customer/tran_fact/daily/tran_fact_2023-01-07.csv', '2023-01-07')]\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-01.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-02/tran_fact_2023-01-01.csv\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-02.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-03/tran_fact_2023-01-02.csv\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-03.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-04/tran_fact_2023-01-03.csv\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-04.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-05/tran_fact_2023-01-04.csv\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-05.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-06/tran_fact_2023-01-05.csv\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-06.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-07/tran_fact_2023-01-06.csv\n",
      "data/src_customer/tran_fact/daily/tran_fact_2023-01-07.csv quintrix-spearscjs data/src_customer/tran_fact/dataset_date=2023-01-08/tran_fact_2023-01-07.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "1.Create table :tran_fact\n",
    "tran_id int, cust_id varchar(20),tran_date date,tran_ammount decimal(10,2), tran_type varchar(1)\n",
    "database name: src_customer\n",
    "tablename: src_transaction\n",
    "partition : 2023-02-01, 2023-02-02,2023-02-03 :ex\n",
    "\n",
    "tran_id : unique id of 6 character\n",
    "cust_id : CA+ 4 integer\n",
    "tran_date: is constant for one file\n",
    "tran_ammount: number havin decimal(10,2)\n",
    "tran_type :[C/D]\n",
    "sate_cd :varchar(2)\n",
    "\n",
    "\n",
    "102020,CA1001,2022-02-01,1200,C,CA\n",
    "102021,CA1002,2022-02-01,700,C,NY\n",
    "102022,CA1003,2022-02-01,500,C,NJ\n",
    "102023,CA1004,2022-02-02,900,C,VA\n",
    "102020,CA1001,2022-02-02,200,D,CA\n",
    "\n",
    "\n",
    "2. Create static partition as dataset_date as varchar(10). Make sure tran_date < dataset_date-1\n",
    "3. Add 7 partitions dataset_date\n",
    "\n",
    "Data stats process:\n",
    "1. create a table as table_states:\n",
    "cols:\n",
    "database_name varchar(20),\n",
    "table_name varchar(50),\n",
    "partition_key varchar(30),\n",
    "rec_count int(10),\n",
    "load_date date,    [[ when are you loading current_time ]]\n",
    "execution_key varchar(100)  >>> Partition col\n",
    "\n",
    "execution_key is (database_name + \"-\"+table_name+\"-\"+partition_key)  [src_customer-tablename-2023-02-01,src_customer-tablename-2023-02-02]\n",
    "\n",
    "\n",
    "2. After each load into tran_fact run the query to load data into table_states\n",
    "query is : get the count from tran_fact an load into table_states\n",
    "\n",
    "insert into table     [[ select from cource table   ]]]]\n",
    "\"\"\" \n",
    "execfile('scripts/hadoop_question/generate_tran_fact_data.py')\n",
    "\n",
    "# see scripts/hadoop_question/hive/load_table_states.sh\n",
    "# see scripts/hadoop_question/hive/init_tran_fact.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "3. Create transaction_fact same as tran_fact in schema cards_dw.\n",
    "you provide path in s3 and add some comment  [[ look how to pass comment in ceating hive database]\n",
    "cols:\n",
    "tablename: transaction_fact   [ stored as parquet and snappy]\n",
    "\n",
    "tran_id :\n",
    "cust_id :\n",
    "tran_date:\n",
    "tran_ammount:\n",
    "tran_type :[\n",
    "load_time: current time   [[ datetime ]]\n",
    "dataset_date ,sate_cd : partition col\n",
    "\"\"\" \n",
    "# see emr/init_tran_fact.sh\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "Build a sql which will read one day data from tran_fact and load into transaction_fact\n",
    "4. Add parameter into sql script which will be passed from command line.\n",
    "\"\"\" \n",
    "# see emr/load_transaction_fact\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "4. Add parameter into sql script which will be passed from command line.\n",
    "\"\"\"\n",
    "# emr/load_transaction_fact.sh [arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "/*\n",
    "Dataquality check:\n",
    "1. create a table as table_states:\n",
    "cols:\n",
    "database_name varchar(20),\n",
    "table_name varchar(50),\n",
    "partition_key varchar(30),\n",
    "rec_count int(10),\n",
    "dataset_date date,\n",
    "execution_key varchar(100)  >>> Partition col\n",
    "\n",
    "execution_key is (database_name + \"-\"+table_name+\"-\"+partition_key\n",
    "\n",
    "\n",
    "2. After each load into tran_fact run the query to load data into table_states\n",
    "query is : get the count from tran_fact an load into table_states\n",
    "\n",
    "*/\n",
    "\"\"\" \n",
    "# see emr/src_customer/ct_table_states.hql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD \n",
    "\"\"\" \n",
    "1.Create table :tran_fact\n",
    "tran_id int, cust_id varchar(20),tran_date date,tran_ammount decimal(10,2), tran_type varchar(1)\n",
    "\n",
    "102020,CA1001,2022-02-01,1200,C\n",
    "102021,CA1002,2022-02-01,700,C\n",
    "102022,CA1003,2022-02-01,500,C\n",
    "102023,CA1004,2022-02-02,900,C\n",
    "102020,CA1001,2022-02-02,200,D\n",
    "102029,CA1001,2022-02-02,700,C\n",
    "102024,CA1005,2022-02-03,12200,C\n",
    "102025,CA1003,2022-02-03,200,D\n",
    "102026,CA1004,2022-02-04,12200,C\n",
    "102027,CA1007,2022-02-04,9200,C\n",
    "102028,CA1007,2022-02-04,3200,D\n",
    "*/\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS hadoop_ingest;\n",
    "DROP TABLE hadoop_ingest.tran_fact;\n",
    "CREATE TABLE hadoop_ingest.tran_fact(  \n",
    "    tran_id int, \n",
    "    cust_id varchar(20),\n",
    "    tran_date date,\n",
    "    tran_ammount decimal(10,2), \n",
    "    tran_type varchar(1)\n",
    ");\n",
    "INSERT INTO hadoop_ingest.tran_fact (tran_id, cust_id, tran_date, tran_ammount, tran_type) VALUES \n",
    "    (102021,'CA1002','2022-02-01',700,'C'),\n",
    "    (102022,'CA1003','2022-02-01',500,'C'),\n",
    "    (102020,'CA1001','2022-02-02',200,'C'),\n",
    "    (102023,'CA1004','2022-02-02',900,'C'),\n",
    "    (102020,'CA1001','2022-02-02',200,'D'),\n",
    "    (102029,'CA1001','2022-02-02',700,'C'),\n",
    "    (102024,'CA1005','2022-02-03',12200,'C'),\n",
    "    (102025,'CA1003','2022-02-03',200,'D'),\n",
    "    (102026,'CA1004','2022-02-04',12200,'C'),\n",
    "    (102027,'CA1007','2022-02-04',9200,'C'),\n",
    "    (102028,'CA1007','2022-02-04',3200,'D'),\n",
    "    (102028,'CA1007','2022-02-05',3200,'C'),\n",
    "    (102029,'CA1007','2022-02-05',3200,'D')\n",
    ";\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30cf0e1abb529a22bc607410d8585599c1f1527cc04e9da2f800037aa994db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
