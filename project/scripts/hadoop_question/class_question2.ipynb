{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' \n",
    "1. Create hadoop architecture diagram.\n",
    "\n",
    "\n",
    "2. Create hive architecture diagram.\n",
    "\n",
    "\n",
    "3. What is difference between dynamic partition vs static partition?\n",
    "static partition -- we load data files into table partition directly via load command\n",
    "dynamic partition -- use column name create partition and insert data\n",
    "\n",
    "4. How to explain plan of hive query?\n",
    "\n",
    "expression tree\n",
    "\n",
    "EXPLAIN\n",
    "FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key;\n",
    "\n",
    "STAGE PLANS:\n",
    "  Stage: Stage-1\n",
    "    Map Reduce\n",
    "      Alias -> Map Operator Tree:\n",
    "        src\n",
    "            Reduce Output Operator\n",
    "              key expressions:\n",
    "                    expr: key\n",
    "                    type: string\n",
    "              sort order: +\n",
    "              Map-reduce partition columns:\n",
    "                    expr: rand()\n",
    "                    type: double\n",
    "              tag: -1\n",
    "              value expressions:\n",
    "                    expr: substr(value, 4)\n",
    "                    type: string\n",
    "      Reduce Operator Tree:\n",
    "        Group By Operator\n",
    "          aggregations:\n",
    "                expr: sum(UDFToDouble(VALUE.0))\n",
    "          keys:\n",
    "                expr: KEY.0\n",
    "                type: string\n",
    "          mode: partial1\n",
    "          File Output Operator\n",
    "            compressed: false\n",
    "            table:\n",
    "                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n",
    "                output format: org.apache.hadoop.mapred.SequenceFileOutputFormat\n",
    "                name: binary_table\n",
    " \n",
    "  Stage: Stage-2\n",
    "    Map Reduce\n",
    "      Alias -> Map Operator Tree:\n",
    "        /tmp/hive-zshao/67494501/106593589.10001\n",
    "          Reduce Output Operator\n",
    "            key expressions:\n",
    "                  expr: 0\n",
    "                  type: string\n",
    "            sort order: +\n",
    "            Map-reduce partition columns:\n",
    "                  expr: 0\n",
    "                  type: string\n",
    "            tag: -1\n",
    "            value expressions:\n",
    "                  expr: 1\n",
    "                  type: double\n",
    "      Reduce Operator Tree:\n",
    "        Group By Operator\n",
    "          aggregations:\n",
    "                expr: sum(VALUE.0)\n",
    "          keys:\n",
    "                expr: KEY.0\n",
    "                type: string\n",
    "          mode: final\n",
    "          Select Operator\n",
    "            expressions:\n",
    "                  expr: 0\n",
    "                  type: string\n",
    "                  expr: 1\n",
    "                  type: double\n",
    "            Select Operator\n",
    "              expressions:\n",
    "                    expr: UDFToInteger(0)\n",
    "                    type: int\n",
    "                    expr: 1\n",
    "                    type: double\n",
    "              File Output Operator\n",
    "                compressed: false\n",
    "                table:\n",
    "                    input format: org.apache.hadoop.mapred.TextInputFormat\n",
    "                    output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\n",
    "                    serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe\n",
    "                    name: dest_g1\n",
    " \n",
    "  Stage: Stage-0\n",
    "    Move Operator\n",
    "      tables:\n",
    "            replace: true\n",
    "            table:\n",
    "                input format: org.apache.hadoop.mapred.TextInputFormat\n",
    "                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\n",
    "                serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe\n",
    "                name: dest_g1\n",
    "\n",
    "\n",
    "there ev_vehicle datasets in data/hive folder.\n",
    "1. Create a external table to store the data.  [Table name: ev_vehicle_info_src]\n",
    "''' \n",
    "# HIVE #######################################################\n",
    "# hive -f sql/cards_ingest/ct_vehicle_info_src.hql\n",
    "# hive -e \"load data inpath 's3://quintrix-spearscjs/cards_ingest/ev_vehicle_info_src/Electric_Vehicle_Population_Data.csv' into table cards_ingest.ev_vehicle_info_src;\"\n",
    "\n",
    "\n",
    "# ATHENA #####################################################\n",
    "'''\n",
    "create external table if not exists cards_ingest.ev_vehicle_info_src_athena ( \n",
    "      vin varchar(10), county varchar(30), city varchar(30), state varchar(2), postal_code varchar(5), model_year int, \n",
    "      make string, model string, electric_vehicle_type string, cafv_eligibility string, electric_range int, base_msrp decimal(12,2), \n",
    "      legislative_district string, dol_vehicle_id string, vehicle_location string, electric_utility string, census_tract_msrp_2020 string ) \n",
    "row format delimited fields terminated by ',' location \"s3://quintrix-spearscjs/data/cards_ingest/ev_vehicle_info_src/\" tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "2. Create another table with partition make.  [ Parquet table with snappy compression][Table name:ev_vehicle_info]\n",
    "\n",
    "'''\n",
    "# HIVE #######################################################\n",
    "# hive -f sql/cards_ingest/ct_vehicle_info.hql\n",
    "# q=\"insert into cards_ingest.ev_vehicle_info partition(make) select vin, county,city,state,postal_code,model_year,model,electric_vehicle_type,\n",
    "#     cafv_eligibility,electric_range,base_msrp,legislative_district,dol_vehicle_id,vehicle_location,electric_utility ,census_tract_msrp_2020, make \n",
    "#     from cards_ingest.ev_vehicle_info_src\"\n",
    "# hive -e $q \n",
    "\n",
    "\n",
    "# ATHENA #####################################################\n",
    "''' \n",
    "CREATE TABLE cards_ingest.ev_vehicle_info_athena \n",
    "WITH(location='s3://quintrix-spearscjs/athena/cards_ingest/ev_vehicle_info_athena/', \n",
    "    partitioned_by=ARRAY['make'], format = 'PARQUET', parquet_compression = 'SNAPPY') \n",
    "AS SELECT vin,county,city,state,postal_code,model_year,model,electric_vehicle_type,\n",
    "     cafv_eligibility,electric_range,base_msrp,legislative_district,dol_vehicle_id,vehicle_location,electric_utility ,\n",
    "     census_tract_msrp_2020, make \n",
    "FROM ev_vehicle_info_src_athena;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "3. Write a sql to show top 3 by number of vehicle registered. [ sql should be in file. the number to pas is dynamic. it can be 3 is can be 5]\n",
    "'''\n",
    "# hive -f sql/cards_ingest/template/top_n_ev_vehicle_info.sql -hivevar n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "4. Find out the vehicle which are Not eligible due to low battery range='Not eligible due to low battery range'\n",
    "''' \n",
    "# hive -e \"select vin, county, city, state, postal_code, model_year, model, cafv_eligibility from cards_ingest.ev_vehicle_info where cafv_eligibility='Not eligible due to low battery range';\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# 5. Write a sql to show top 3 by Write a sql to show top 3 by n (in sql file parameter top n) (take out for now)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. write a sql to show how many distinct model for each maker.\n",
    "''' \n",
    "# hive -e 'select make, count(distinct model) num_distinct_models from cards_ingest.ev_vehicle_info GROUP BY make;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "7.Write sql to count number of vehicle where Model Year < 2018 and Not eligible due to low battery range='Not eligible due to low battery range'\n",
    "''' \n",
    "# hive -e \"select count(*) from cards_ingest.ev_vehicle_info WHERE CAST(model_year as INT) < 2018 AND  cafv_eligibility='Not eligible due to low battery range';\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "AFTER DO THIS DO THE SAME THING IN ATHENA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FILE FOR CLASS QUESTION 2 BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import db\n",
    "from utils import s3\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1.Create table :tran_fact\\ntran_id int, cust_id varchar(20),tran_ammount decimal(10,2), tran_type varchar(1), tran_country_cd varchar(3),tran_date date\\ntran_id : random id (ex : FX_123456,TD_224452  (Concat [FX/TD] with 6 digit number)\\ncust_id : cust_12345 (concat cust+ 4 digit number)\\ntran_ammount : $ ammount ( 3 dgit to 10 digit number)\\ntran_type : C/D (Random Value)\\ncountry_cd : ['USA','CAN','IND','AFG','CHN','JPN','KON','PAL']\\ntran_date : ['2022-01-01' to '2022-03-10' ]\\n\\nGenerate records for every day with the date range given [ each day generate 1000 to 10000 records]\\nhave 7 days fro now in tester\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "1.Create table :tran_fact\n",
    "tran_id int, cust_id varchar(20),tran_ammount decimal(10,2), tran_type varchar(1), tran_country_cd varchar(3),tran_date date\n",
    "tran_id : random id (ex : FX_123456,TD_224452  (Concat [FX/TD] with 6 digit number)\n",
    "cust_id : cust_12345 (concat cust+ 4 digit number)\n",
    "tran_ammount : $ ammount ( 3 dgit to 10 digit number)\n",
    "tran_type : C/D (Random Value)\n",
    "country_cd : ['USA','CAN','IND','AFG','CHN','JPN','KON','PAL']\n",
    "tran_date : ['2022-01-01' to '2022-03-10' ]\n",
    "\n",
    "Generate records for every day with the date range given [ each day generate 1000 to 10000 records]\n",
    "have 7 days fro now in tester\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "CREATE SCHEMA IF NOT EXISTS hadoop;\n",
    "DROP TABLE hadoop.tran_fact;\n",
    "CREATE TABLE IF NOT EXISTS hadoop.tran_fact(\n",
    "    tran_id varchar(18) encode raw, \n",
    "    cust_id varchar(20) encode raw,\n",
    "    tran_ammount decimal(10,2) encode AZ64, \n",
    "    tran_type varchar(1) encode raw, \n",
    "    tran_country_cd varchar(3) encode bytedict,\n",
    "    tran_date date encode delta32k\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# def generate_dummy_data(start: datetime, num_days : int, num_records : int ):\n",
    "    \n",
    "start_date = datetime.datetime(2022, 1, 1)\n",
    "num_days = 7\n",
    "num_records = 5000\n",
    "\n",
    "\n",
    "def generate_dummy_data(start: datetime, num_days : int, num_records : int, filepath = 'data/hadoop/'):\n",
    "    tran_country_cds = ['USA','CAN','IND','AFG','CHN','JPN','KON','PAL']\n",
    "    tran_types = ['C', 'D']\n",
    "    id_prefixes = ['FX', 'TD']\n",
    "    header = 'tran_id,cust_id,tran_ammount,tran_type,tran_country_cd,tran_date\\n'\n",
    "    files_created = []\n",
    "    for i in range(num_days):\n",
    "        tran_date = start_date + datetime.timedelta(days=i)\n",
    "        filename = tran_date.strftime(\"tran_fact_%Y-%m-%d\") + \".csv\"\n",
    "        f = open(filepath + filename, 'w')\n",
    "        f.write(header)\n",
    "        for j in range(num_records):\n",
    "            tran_ammount = round(random.uniform(100.00, 99999999.99),2)\n",
    "            tran_type = random.choice(tran_types)\n",
    "            tran_country_cd = random.choice(tran_country_cds)\n",
    "            tran_id = random.choice(id_prefixes) + '_' + tran_date.strftime(\"%Y%m%d\") + '_' +  f\"{j+1:06}\"\n",
    "            cust_id = 'cust_' + f\"{random.randint(1, 9999):04}\"\n",
    "            f.write(tran_id + \",\" + cust_id + \",\" + str(tran_ammount) + \",\"\n",
    "                + tran_type + \",\" + tran_country_cd + \",\" + tran_date.strftime(\"%Y-%m-%d\") + '\\n')\n",
    "        files_created.append((filename, filepath, tran_date.strftime('%Y-%m-%d')))\n",
    "        f.close()\n",
    "    return files_created\n",
    "        \n",
    "files_created = generate_dummy_data(start_date, num_days, num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "2. Load each file into s3. [ each with folder as 2022-01-01,2022-01-02]\n",
    "'''\n",
    "for f in files_created:\n",
    "    s3.upload_file(f[1] + f[0], 'quintrix-spearscjs', 'cards_ingest/tran_fact/daily/' + f[2] + '/' + f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql/create/ct_hadoop_tran_fact.sql done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3.Create table in Redshift.\n",
    "'''\n",
    "query = 'CREATE SCHEMA IF NOT EXISTS hadoop'\n",
    "db.do_query(query, [])\n",
    "db.do_query_file('sql/create/ct_hadoop_tran_fact.sql',[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. Copy data to redshift  [append mode]\n",
    "'''\n",
    "db.copy('tran_fact', 'hadoop',  's3://quintrix-spearscjs/hadoop/data/daily/', db.redshift_iam, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Unload query sum(tran)amt by country cd to parquet file in s3.\n",
    "'''\n",
    "subquery = 'SELECT tran_country_cd, SUM(tran_ammount) FROM hadoop.tran_fact GROUP BY tran_country_cd;'\n",
    "bucket = 's3://quintrix-spearscjs/hadoop/data/country/tran_ammt_sum/'\n",
    "query = \"\"\" \n",
    "    UNLOAD (%s) -- query string\n",
    "    TO %s   -- s3 bucket_name\n",
    "    IAM_ROLE %s -- redshift iam role\n",
    "    PARALLEL ON\n",
    "    ALLOWOVERWRITE \n",
    "    FORMAT PARQUET\n",
    "\"\"\"\n",
    "db.do_query(query, [subquery, bucket, db.redshift_iam])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30cf0e1abb529a22bc607410d8585599c1f1527cc04e9da2f800037aa994db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
